---
# This will eventually become the Ansible equivalent of:
# knife ec2 server create --policy-group failover --policy-name primary-web-server
# -I ami-0fc5d935ebf8bc3bc --ssh-key standard -f t2.medium --connection-user ubuntu
# --ebs-size 32 --sudo --ssh-identity-file ~/.ssh/ec2.pem --security-group-id
# sg-245c9940 --associate-eip 34.206.218.126 --server-connect-attribute
# public_ip_address --subnet subnet-71b71628 --ebs-volume-type gp2
# --node-name cally --ssh-verify-host-key never

# Can be used in one fell swoop with our main configuration

- name: Bootstrap AWS Instance
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    create_recovery_volume: false
    instances:
      - name: ansible-test-dev0
        group: web
        env: dev
        eip: 34.198.66.246

  pre_tasks:

  tasks:
    # This will find an existing instance by name, or create it if it
    # doesn't exist.
    - name: Start an instance and add EBS.
      connection: local
      amazon.aws.ec2_instance:
        name: "{{ item.name }}"
        tags:
          group: "{{ item.group }}"
          env: "{{ item.env }}"
        state: running
        instance_type: t2.medium
        # image_id: ami-04552bb4f4dd38925 # 22.04 20241120
        image_id: ami-00f3c44a2de45a590 # 24.04 20241206
        key_name: ansible # We've uploaded this public key to Amazon already
        volumes:
          - device_name: /dev/sda1
            ebs:
              volume_size: 32
              delete_on_termination: false
        security_group: sg-245c9940
        vpc_subnet_id: subnet-71b71628
        network:
          assign_public_ip: true
      register: newinstances
      with_items: "{{ instances }}"

    - name: Debug instance data
      connection: local
      ansible.builtin.debug:
        var: newinstances.results

    - name: Associate an elastic IP with an instance, if desired
      connection: local
      amazon.aws.ec2_eip:
        device_id: "{{ item.instances[0].instance_id }}"
        ip: "{{ item.item.eip }}"
      with_items: "{{ newinstances.results }}"
      when: item.item.eip is defined

    - name: Add volume for /www
      amazon.aws.ec2_vol:
        instance: "{{ item.instances[0].instance_id }}"
        name: "www-{{ item.item.name }}"
        device_name: /dev/sdf
        volume_size: 64
        volume_type: gp3 # ssd, cheaper than gp2 and the differences are inconsequential for us
      register: ec2_vol_www
      with_items: "{{ newinstances.results }}"

    - name: Add volume for /var/lib/mysql
      amazon.aws.ec2_vol:
        instance: "{{ item.instances[0].instance_id }}"
        name: "db-{{ item.item.name }}"
        device_name: /dev/sdg
        volume_size: 8
        volume_type: gp3
      register: ec2_vol_db
      with_items: "{{ newinstances.results }}"

    - name: Add volume for /data/recovery
      amazon.aws.ec2_vol:
        instance: "{{ item.instances[0].instance_id }}"
        name: "recovery-{{ item.item.name }}"
        device_name: /dev/sdh
        volume_size: 50
        volume_type: gp3
      register: ec2_vol_recovery
      with_items: "{{ newinstances.results }}"
      when: create_recovery_volume

    - name: Add new hosts to inventory for use below
      # For now we'll just set the facts up to connect via the public
      # DNS name, which should be fine for the rest of this playbook.
      # Our dymanic inventory can do something different; it doesn't
      # really matter.
      ansible.builtin.add_host:
        name: "{{ item.instances[0].tags.Name }}"
        groups: "new,{{ item.instances[0].tags.group }}"
        ansible_user: ubuntu
        # We use the EIP for preference, as we want to use the wait_for_connection
        # below to validate that the EIP is assigned and working before moving on.
        # If we haven't assigned an EIP, then we just use the public IP. (We can't
        # *just* use the Public IP as it gets overwritten with the EIP when assigned,
        # and that might be happening *right now*.)
        ansible_host: "{{ item.item.eip | default(item.instances[0].public_ip_address) }}"
        ansible_ssh_private_key_file: ~/.ssh/ansible
      with_items: "{{ newinstances.results }}"

- name: "Now on to the hosts!"
  hosts: new
  become: true
  gather_facts: false

  tasks:
    - name: Wait for hosts to become reachable.
      ansible.builtin.wait_for_connection:

    - name: Set hostname
      ansible.builtin.hostname:
        name: "{{ inventory_hostname }}"

    - name: Gather facts afresh now we can connect
      ansible.builtin.setup:

    - name: Refresh inventory so we start with a clean slate from our dynamic inventory
      ansible.builtin.meta: refresh_inventory

    - name: Create mount points
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      with_items:
        - '/var/www'
        - '/var/lib/mysql'
        - '/data/recovery'

    - name: Filesystem for www
      community.general.filesystem:
        fstype: ext4
        state: present
        dev: /dev/xvdf

    - name: Filesystem for db
      community.general.filesystem:
        fstype: ext4
        state: present
        dev: /dev/xvdg

    - name: Filesystem for recovery
      community.general.filesystem:
        fstype: ext4
        state: present
        dev: /dev/xvdh
      when: create_recovery_volume

    - name: Mount www
      ansible.posix.mount:
        path: /var/www
        state: mounted
        fstype: ext4
        src: /dev/xvdf
        opts: noatime,auto,nofail

    - name: Mount db
      ansible.posix.mount:
        path: /var/lib/mysql
        state: mounted
        fstype: ext4
        src: /dev/xvdg
        opts: noatime,auto,nofail

    - name: Mount '/data/recovery'
      ansible.posix.mount:
        path: '/data/recovery'
        state: mounted
        fstype: ext4
        src: '/dev/xvdh'
        opts: noatime,auto,nofail
      when: create_recovery_volume

# Okay, now we're ready to get on with things via our dynamic inventory using
# a more normal playbook. You can run provision.yml in the root of the project
# to call this script immeidately followed by the standard playbook.
